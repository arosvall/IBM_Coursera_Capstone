{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
     ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "The dataset was adapted from the Wine Quality Dataset (https://archive.ics.uci.edu/ml/datasets/Wine+Quality)\n",
    "\n",
    "### Attribute Information:\n",
    "\n",
    "For more information, read [Cortez et al., 2009: http://dx.doi.org/10.1016/j.dss.2009.05.016].\n",
    "\n",
    "Input variables (based on physicochemical tests):\n",
    "\n",
    "    1 - fixed acidity \n",
    "    2 - volatile acidity \n",
    "    3 - citric acid \n",
    "    4 - residual sugar \n",
    "    5 - chlorides \n",
    "    6 - free sulfur dioxide \n",
    "    7 - total sulfur dioxide \n",
    "    8 - density \n",
    "    9 - pH \n",
    "    10 - sulphates \n",
    "    11 - alcohol \n",
    "Output variable (based on sensory data):\n",
    "\n",
    "    12 - quality (0: normal wine, 1: good wine)\n",
    "    \n",
    "## Problem statement\n",
    "Predict the quality of a wine given its input variables. Use AUC (area under the receiver operating characteristic curve) as the evaluation metric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's load and explore the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "np.random.seed = 42\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed_acidity</th>\n",
       "      <th>volatile_acidity</th>\n",
       "      <th>citric_acid</th>\n",
       "      <th>residual_sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free_sulfur_dioxide</th>\n",
       "      <th>total_sulfur_dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.0</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.36</td>\n",
       "      <td>20.7</td>\n",
       "      <td>0.045</td>\n",
       "      <td>45.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>1.0010</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0.45</td>\n",
       "      <td>8.8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6.3</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.34</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.049</td>\n",
       "      <td>14.0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>0.9940</td>\n",
       "      <td>3.30</td>\n",
       "      <td>0.49</td>\n",
       "      <td>9.5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.1</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.40</td>\n",
       "      <td>6.9</td>\n",
       "      <td>0.050</td>\n",
       "      <td>30.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>0.9951</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.44</td>\n",
       "      <td>10.1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.2</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.32</td>\n",
       "      <td>8.5</td>\n",
       "      <td>0.058</td>\n",
       "      <td>47.0</td>\n",
       "      <td>186.0</td>\n",
       "      <td>0.9956</td>\n",
       "      <td>3.19</td>\n",
       "      <td>0.40</td>\n",
       "      <td>9.9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.2</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.32</td>\n",
       "      <td>8.5</td>\n",
       "      <td>0.058</td>\n",
       "      <td>47.0</td>\n",
       "      <td>186.0</td>\n",
       "      <td>0.9956</td>\n",
       "      <td>3.19</td>\n",
       "      <td>0.40</td>\n",
       "      <td>9.9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fixed_acidity  volatile_acidity  citric_acid  residual_sugar  chlorides  \\\n",
       "0            7.0              0.27         0.36            20.7      0.045   \n",
       "1            6.3              0.30         0.34             1.6      0.049   \n",
       "2            8.1              0.28         0.40             6.9      0.050   \n",
       "3            7.2              0.23         0.32             8.5      0.058   \n",
       "4            7.2              0.23         0.32             8.5      0.058   \n",
       "\n",
       "   free_sulfur_dioxide  total_sulfur_dioxide  density    pH  sulphates  \\\n",
       "0                 45.0                 170.0   1.0010  3.00       0.45   \n",
       "1                 14.0                 132.0   0.9940  3.30       0.49   \n",
       "2                 30.0                  97.0   0.9951  3.26       0.44   \n",
       "3                 47.0                 186.0   0.9956  3.19       0.40   \n",
       "4                 47.0                 186.0   0.9956  3.19       0.40   \n",
       "\n",
       "   alcohol  quality  \n",
       "0      8.8        0  \n",
       "1      9.5        0  \n",
       "2     10.1        0  \n",
       "3      9.9        0  \n",
       "4      9.9        0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"whitewine.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4715 entries, 0 to 4714\n",
      "Data columns (total 12 columns):\n",
      "fixed_acidity           4715 non-null float64\n",
      "volatile_acidity        4715 non-null float64\n",
      "citric_acid             4715 non-null float64\n",
      "residual_sugar          4715 non-null float64\n",
      "chlorides               4715 non-null float64\n",
      "free_sulfur_dioxide     4715 non-null float64\n",
      "total_sulfur_dioxide    4715 non-null float64\n",
      "density                 4715 non-null float64\n",
      "pH                      4715 non-null float64\n",
      "sulphates               4715 non-null float64\n",
      "alcohol                 4715 non-null float64\n",
      "quality                 4715 non-null int64\n",
      "dtypes: float64(11), int64(1)\n",
      "memory usage: 442.1 KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    3655\n",
       "1    1060\n",
       "Name: quality, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"quality\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please note that this dataset is unbalanced."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions and Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[1]. Split the given data using stratify sampling into 2 subsets: training (80%) and test (20%) sets. Use random_state = 42. [1 points]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X = data\n",
    "y = data['quality']\n",
    "X.drop(['quality'], axis=1, inplace = True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3772, 11)\n",
      "(3772,)\n",
      "(943, 11)\n",
      "(943,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[2]. Use ``GridSearchCV`` and ``Pipeline`` to tune hyper-parameters for 3 different classifiers including ``KNeighborsClassifier``, ``LogisticRegression`` and ``svm.SVC`` and report the corresponding AUC values on the training and test sets. Note that a scaler may need to be inserted into each pipeline. [6 points]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "KNN_parameters = {'knn__n_neighbors': (1,3,5,7,10), 'knn__weights':('uniform', 'distance')}\n",
    "LR_parameters = {'lr__C':[0.5, 1.0, 2.0, 3.0, 4.5, 6.0], 'lr__solver':('lbfgs', 'liblinear','saga')}\n",
    "SVM_parameters = {'svm__C': [0.5, 1.0, 2.0, 3.0, 4.5, 6.0], 'svm__kernel':('linear', 'sigmoid', 'rbf')}\n",
    "\n",
    "\n",
    "SVM_Pipeline = Pipeline(steps = [('scale', StandardScaler()), ('svm', SVC(probability=True))])\n",
    "KNN_Pipeline = Pipeline(steps = [('scale', StandardScaler()), ('knn', KNeighborsClassifier())])\n",
    "LR_Pipeline =  Pipeline(steps = [('scale', StandardScaler()), ('lr', LogisticRegression(max_iter=1000))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN training score= 1.0\n",
      "KNN test score= 0.8127339132230338\n"
     ]
    }
   ],
   "source": [
    "KNN_GSCV = GridSearchCV(KNN_Pipeline, param_grid = KNN_parameters, cv=3, scoring='roc_auc')\n",
    "KNN_GSCV.fit(X_train, y_train)\n",
    "\n",
    "train_Pred = KNN_GSCV.predict(X_train)\n",
    "test_Pred = KNN_GSCV.predict(X_test)\n",
    "\n",
    "knn_train_auc = roc_auc_score(y_train, train_Pred)\n",
    "print(\"KNN training score= \" + str(knn_train_auc))\n",
    "knn_test_auc = roc_auc_score(y_test, test_Pred)\n",
    "print(\"KNN test score= \" + str(knn_test_auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR training score= 0.6133712864259351\n",
      "LR test score= 0.5962722298221614\n"
     ]
    }
   ],
   "source": [
    "LR_GSCV = GridSearchCV(LR_Pipeline, param_grid = LR_parameters, cv=3, scoring='roc_auc')\n",
    "LR_GSCV.fit(X_train, y_train)\n",
    "\n",
    "train_Pred = LR_GSCV.predict(X_train)\n",
    "test_Pred = LR_GSCV.predict(X_test)\n",
    "\n",
    "lr_train_auc = roc_auc_score(y_train, train_Pred)\n",
    "print(\"LR training score= \" + str(lr_train_auc))\n",
    "lr_test_auc = roc_auc_score(y_test, test_Pred)\n",
    "print(\"LR test score= \" + str(lr_test_auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM training score= 0.7504533076942932\n",
      "SVM test score= 0.6845978628397388\n"
     ]
    }
   ],
   "source": [
    "SVM_GSCV = GridSearchCV(SVM_Pipeline, param_grid = SVM_parameters, cv=3, scoring='roc_auc')\n",
    "SVM_GSCV.fit(X_train, y_train)\n",
    "\n",
    "train_Pred = SVM_GSCV.predict(X_train)\n",
    "test_Pred = SVM_GSCV.predict(X_test)\n",
    "\n",
    "svm_train_auc = roc_auc_score(y_train, train_Pred)\n",
    "print(\"SVM training score= \" + str(svm_train_auc))\n",
    "svm_test_auc = roc_auc_score(y_test, test_Pred)\n",
    "print(\"SVM test score= \" + str(svm_test_auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training score = 0.7879415313734094\n",
      "Average test score = 0.6978680019616448\n"
     ]
    }
   ],
   "source": [
    "ave_train = (svm_train_auc + knn_train_auc + lr_train_auc) / 3\n",
    "ave_test = (svm_test_auc + knn_test_auc + lr_test_auc) / 3\n",
    "print(\"Average training score = \" + str(ave_train))\n",
    "print(\"Average test score = \" + str(ave_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[3]. Train a soft ``VotingClassifier`` with the estimators are the three tuned pipelines obtained from [2]. Report the AUC values on the training and test sets. Comment on the performance of the ensemble model. [2 points]**\n",
    "\n",
    "Document: https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html#sklearn.ensemble.VotingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Voting Classifier training auc = 0.9692451107221673\n",
      "Voting Classifier test auc = 0.8578478352248844\n"
     ]
    }
   ],
   "source": [
    "vc = VotingClassifier(estimators=[('lr', LR_GSCV), ('knn', KNN_GSCV), ('svm',SVM_GSCV)], voting='soft')\n",
    "vc.fit(X_train, y_train)\n",
    "\n",
    "train_preds = vc.predict(X_train)\n",
    "train_auc = roc_auc_score(train_preds, y_train)\n",
    "print(\"Voting Classifier training auc = \" + (str(train_auc)))\n",
    "test_preds = vc.predict(X_test)\n",
    "test_auc = roc_auc_score(test_preds, y_test)\n",
    "print(\"Voting Classifier test auc = \" + (str(test_auc)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performance of the Voting Classifier model is superior to all of the three individual models.The performance auc improves from the highest value of 0.81 up to 0.85 ad reduces the overfitting of the training data that occurs in the knn model , and the difference between the training and test aucs is not strong enough to suggest major bias towards the training data. Using the soft voting gives a higher auc score than the hard voting, which is based on the optimisation of the models which occurs in the grid search cv models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[4]. Redo [3] with a sensible set of ``weights`` for the estimators. Comment on the performance of the ensemble model in this case. [1 point]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Voting Classifier training auc = 1.0\n",
      "Voting Classifier test auc = 0.8531247705074538\n"
     ]
    }
   ],
   "source": [
    "weights = [1,4,2]\n",
    "weighted_vc = VotingClassifier(estimators=[('lr', LR_GSCV), ('knn', KNN_GSCV), ('svm',SVM_GSCV)], weights=weights, voting='soft')\n",
    "weighted_vc.fit(X_train, y_train)\n",
    "\n",
    "train_preds = weighted_vc.predict(X_train)\n",
    "train_auc = roc_auc_score(train_preds, y_train)\n",
    "print(\"Voting Classifier training auc = \" + (str(train_auc)))\n",
    "test_preds = weighted_vc.predict(X_test)\n",
    "test_auc = roc_auc_score(test_preds, y_test)\n",
    "print(\"Voting Classifier test auc = \" + (str(test_auc)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second, weighted model of the voting classifier has an improvement in training AUC, though the gap between the two AUC scores is wider which suggests that there is more overfitting on the training data. The weights are stacked so that the KNN model, which was the most accurate, has the strongest weight, while the logisitic regression model has the weakest weights and thus the least influence on the ensemble model. The weighted model does not perform better than the unweighted model. The performance of the model is superior to the KNN model, matching the training AUC score but improving on the test AUC score. Heavier weights to the KNN model in the voting classifier did not improve the scores, while lower weights lessened the difference between the training and testing scores but also lowered those scores overall. The unweighted model has a very similar test score but a lower training, which suggests that the weighted model is overfitting to the training data and producing an unnecessarily high roc score on this data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[5][*Optional - for bonus points only*]. Use the ``VotingClassifier`` with ``GridSearchCV`` to tune the hyper-parameters of the individual estimators. The parameter grid should be a combination of those in [2]. Report the AUC values on the training and test sets. Comment on the performance of the ensemble model. [2 points]**\n",
    "\n",
    "Document: https://scikit-learn.org/stable/modules/ensemble.html#using-the-votingclassifier-with-gridsearchcv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
