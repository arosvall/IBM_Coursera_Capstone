{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part Two Assignment Four"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This notebook contains all the code for assignment four part two. There is EDA and each of the ten algorithms in this code, along with data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The majority of the explanation of this data analysis is within the report under Part_Two.pdf. The point of this analysis is to identify missing values, outliers, and features which need to be transformed from strings to binary. Later in the code there is a limited exploration of categorical data such as married status and the implication of it on the income class, showing relationships or lack there of between features and classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('adult.data', names =  [\"Age\", \"Workclass\", \"Final_Weight\", \"Education\", \"Education_Num\", \"Marital_Status\", \"Occupation\", \"Relationship\", \"Race\", \"Sex\", \"Capital_Gain\", \"Capital_Loss\", \"Hours_per_Week\", \"Native_Country\", \"Income_Class\"], header = None )\n",
    "test_df = pd.read_csv('adult.test', names =  [\"Age\", \"Workclass\", \"Final_Weight\", \"Education\", \"Education_Num\", \"Marital_Status\", \"Occupation\", \"Relationship\", \"Race\", \"Sex\", \"Capital_Gain\", \"Capital_Loss\", \"Hours_per_Week\", \"Native_Country\", \"Income_Class\"], header = None )\n",
    "\n",
    "train_df.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = test_df.drop(test_df.index[0], axis = 0)\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in train_df.columns:\n",
    "    print(column, \":\", train_df[column].unique(), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace ? values with Nones to remove easily, then merge similar feature values e.g. married-civ-spouse, married-spouse-absent, to simply married. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.replace(' ?', np.NaN, inplace = True)\n",
    "test_df.replace(' ?', np.NaN, inplace = True)\n",
    "\n",
    "train_df.dropna(axis = 0, inplace = True)\n",
    "test_df.dropna(axis = 0, inplace = True)\n",
    "test_df.isnull().values.any()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['Marital_Status']= np.where(train_df['Marital_Status'] == ' Married-civ-spouse', ' Married', train_df['Marital_Status'])\n",
    "train_df['Marital_Status']= np.where(train_df['Marital_Status'] == ' Married-AF-spouse', ' Married', train_df['Marital_Status'])\n",
    "train_df['Marital_Status']= np.where(train_df['Marital_Status'] == ' Married-spouse-absent', ' Married', train_df['Marital_Status'])\n",
    "\n",
    "test_df['Marital_Status']= np.where(test_df['Marital_Status'] == ' Married-civ-spouse', ' Married', test_df['Marital_Status'])\n",
    "test_df['Marital_Status']= np.where(test_df['Marital_Status'] == ' Married-AF-spouse', ' Married', test_df['Marital_Status'])\n",
    "test_df['Marital_Status']= np.where(test_df['Marital_Status'] == ' Married-spouse-absent', ' Married', test_df['Marital_Status'])\n",
    "\n",
    "train_df['Marital_Status'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['Education']= np.where(train_df['Education'] == ' Preschool', ' School_Leaver', train_df['Education'])\n",
    "train_df['Education']= np.where(train_df['Education'] == ' 1st-4th', ' School_Leaver', train_df['Education'])\n",
    "train_df['Education']= np.where(train_df['Education'] == ' 5th-6th', ' School_Leaver', train_df['Education'])\n",
    "train_df['Education']= np.where(train_df['Education'] == ' 7th-8th', ' School_Leaver', train_df['Education'])\n",
    "train_df['Education']= np.where(train_df['Education'] == ' 9th', ' School_Leaver', train_df['Education'])\n",
    "train_df['Education']= np.where(train_df['Education'] == ' 10th', ' School_Leaver', train_df['Education'])\n",
    "train_df['Education']= np.where(train_df['Education'] == ' 11th', ' School_Leaver', train_df['Education'])\n",
    "train_df['Education']= np.where(train_df['Education'] == ' 12th', ' School_Leaver', train_df['Education'])\n",
    "\n",
    "test_df['Education']= np.where(test_df['Education'] == ' Preschool', ' School_Leaver', test_df['Education'])\n",
    "test_df['Education']= np.where(test_df['Education'] == ' 1st-4th', ' School_Leaver', test_df['Education'])\n",
    "test_df['Education']= np.where(test_df['Education'] == ' 5th-6th', ' School_Leaver', test_df['Education'])\n",
    "test_df['Education']= np.where(test_df['Education'] == ' 7th-8th', ' School_Leaver', test_df['Education'])\n",
    "test_df['Education']= np.where(test_df['Education'] == ' 9th', ' School_Leaver', test_df['Education'])\n",
    "test_df['Education']= np.where(test_df['Education'] == ' 10th', ' School_Leaver', test_df['Education'])\n",
    "test_df['Education']= np.where(test_df['Education'] == ' 11th', ' School_Leaver', test_df['Education'])\n",
    "test_df['Education']= np.where(test_df['Education'] == ' 12th', ' School_Leaver', test_df['Education'])\n",
    "\n",
    "train_df['Education'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['Workclass'] = np.where(train_df['Workclass'] == ' Self-emp-inc', ' Self-emp', train_df['Workclass'])\n",
    "train_df['Workclass'] = np.where(train_df['Workclass'] == ' Self-emp-not-inc', ' Self-emp', train_df['Workclass'])\n",
    "train_df['Workclass'] = np.where(train_df['Workclass'] == ' Local-gov', ' Gov', train_df['Workclass'])\n",
    "train_df['Workclass'] = np.where(train_df['Workclass'] == ' State-gov', ' Gov', train_df['Workclass'])\n",
    "train_df['Workclass'] = np.where(train_df['Workclass'] == ' Federal-gov', ' Gov', train_df['Workclass'])\n",
    "\n",
    "\n",
    "test_df['Workclass'] = np.where(test_df['Workclass'] == ' Self-emp-inc', ' Self-emp', test_df['Workclass'])\n",
    "test_df['Workclass'] = np.where(test_df['Workclass'] == ' Self-emp-not-inc', ' Self-emp', test_df['Workclass'])\n",
    "test_df['Workclass'] = np.where(test_df['Workclass'] == ' Local-gov', ' Gov', test_df['Workclass'])\n",
    "test_df['Workclass'] = np.where(test_df['Workclass'] == ' State-gov', ' Gov', test_df['Workclass'])\n",
    "test_df['Workclass'] = np.where(test_df['Workclass'] == ' Federal-gov', ' Gov', test_df['Workclass'])\n",
    "\n",
    "train_df['Workclass'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['Relationship'] = np.where(train_df['Relationship'] == ' Wife', ' Spouse', train_df['Relationship'])\n",
    "train_df['Relationship'] = np.where(train_df['Relationship'] == ' Husband', ' Spouse', train_df['Relationship'])\n",
    "\n",
    "test_df['Relationship'] = np.where(test_df['Relationship'] == ' Wife', ' Spouse', test_df['Relationship'])\n",
    "test_df['Relationship'] = np.where(test_df['Relationship'] == ' Husband', ' Spouse', test_df['Relationship'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Description of the numerical features for analysis of outliers / distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_df.hist(bins = 10, figsize = (14, 10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classes are unbalanced, Most ages around around 20-50, Capital gain outlier = 99999, removing before continuing with analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove the outlier\n",
    "train_df.drop(train_df[train_df['Capital_Gain'] == 99999].index, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in train_df.columns:\n",
    "    print(column, \":\", train_df[column].unique(), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the split of the unbalanced data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.Income_Class.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.groupby(\"Income_Class\").mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that Capital Gain has the largest difference, along with Capital loss. More hours per week tend to relate to more money earned, all variables increase as pay increases. Rich people pay more for their assets but tend to lose more money when they sell said assets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.groupby(\"Age\").mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(train_df.Age, train_df.Income_Class).plot(kind = 'bar', figsize = (20,10))\n",
    "plt.title('Income Class Frequency Per Age')\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Income Class Frequency')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Age does not appear to have any patterns as it increases. The majority of the high work hours are between the ages of 20 and 65, as is expected from the retirement age of 65, and there is no clear pattern wtih the capital loss/gain measures. There is a trend between age and the class prediction however, as older ages tend to have a larger number of higher income classes, especially between the ages of 37-60 where the split between the two classes is more even than the younger ages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.groupby(\"Education\").mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(train_df.Education, train_df.Income_Class).plot(kind = 'bar', figsize = (20,10), stacked = True)\n",
    "plt.title('Income Class Frequency per Education')\n",
    "plt.xlabel('Education')\n",
    "plt.ylabel('Income Class Frequency')\n",
    "#shows useless "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.groupby(\"Occupation\").mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(train_df.Occupation, train_df.Income_Class).plot(kind = 'bar', figsize = (20,10), stacked = True)\n",
    "plt.title('Income Class Frequency Per Occupation')\n",
    "plt.xlabel('Occupation')\n",
    "plt.ylabel('Income Class Frequency')\n",
    "#shows useless "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.groupby(\"Hours_per_Week\").mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(train_df.Hours_per_Week, train_df.Income_Class).plot(kind = 'bar', figsize = (20,10))\n",
    "plt.title('Income Class Frequency Per Hours Worked')\n",
    "plt.xlabel('Hours worked')\n",
    "plt.ylabel('Income Class Frequency')\n",
    "#shows useless "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.groupby(\"Sex\").mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(train_df.Sex, train_df.Income_Class).plot(kind = 'bar', figsize = (20,10), stacked = True)\n",
    "plt.title('Income Class Frequency Per Hours Worked')\n",
    "plt.xlabel('Gender')\n",
    "plt.ylabel('Income Class Frequency')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(train_df.Marital_Status, train_df.Income_Class).plot(kind = 'bar', figsize = (20,10), stacked = True)\n",
    "plt.title('Income Class Frequency Per Marital Status')\n",
    "plt.xlabel('Marital Status')\n",
    "plt.ylabel('Income Class Frequency')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.groupby(\"Education_Num\").mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(train_df.Education_Num, train_df.Income_Class).plot(kind = 'bar', figsize = (20,10))\n",
    "plt.title('Income Class Frequency Per Education Number')\n",
    "plt.xlabel('Education Number')\n",
    "plt.ylabel('Income Class Frequency')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below replacing the string values of classes with 0 if <=50K, 1 if >50K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.replace(' <=50K', 0, inplace = True)\n",
    "train_df.replace(' >50K', 1, inplace = True)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.replace(' <=50K.', 0, inplace = True)\n",
    "test_df.replace(' >50K.', 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Pre-Processing, splitting the categorical features into binary encoding. The following code is mostly taken from the tutorial given on classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = train_df['Income_Class']\n",
    "train_df.drop(['Income_Class'], axis = 1, inplace = True)\n",
    "tb = train_df.copy()\n",
    "column_vars = [\"Workclass\", \"Education\", \"Marital_Status\", \"Occupation\", \"Relationship\", \"Race\", \"Sex\", \"Native_Country\"]\n",
    "for var in column_vars:\n",
    "    col_list='var' + '_' + var\n",
    "    col_list = pd.get_dummies(train_df[var], prefix =var)\n",
    "    data1 = train_df.join(col_list)\n",
    "    train_df = data1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_vars = train_df.columns.values.tolist()\n",
    "keep = [i for i in data_vars if i not in column_vars]\n",
    "train_df_final = train_df[keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_vars = train_df_final.columns.values.tolist()\n",
    "train_index = [i for i in df_final_vars if i not in train_labels]\n",
    "train_df = train_df_final[train_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels = test_df['Income_Class']\n",
    "test_df.drop(['Income_Class'], axis = 1, inplace = True)\n",
    "column_vars = [\"Workclass\", \"Education\", \"Marital_Status\", \"Occupation\", \"Relationship\", \"Race\", \"Sex\", \"Native_Country\"]\n",
    "for var in column_vars:\n",
    "    col_list='var' + '_' + var\n",
    "    col_list = pd.get_dummies(tb[var], prefix =var)\n",
    "    data1 = test_df.join(col_list)\n",
    "    test_df = data1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_vars = test_df.columns.values.tolist()\n",
    "keep = [i for i in data_vars if i not in column_vars]\n",
    "\n",
    "test_df_final = test_df[keep]\n",
    "\n",
    "df_final_vars = test_df_final.columns.values.tolist()\n",
    "test_index = [i for i in df_final_vars if i not in test_labels]\n",
    "test_df = test_df_final[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.head()\n",
    "test_df.replace(np.NaN, 0, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PCA to reduce dimensionality, not used in final model but kept in code as was considered \n",
    "#from sklearn.decomposition import PCA\n",
    "#pca = PCA(n_components = 25)\n",
    "#pca.fit(df)\n",
    "#columns = ['pca_%i' % i for i in range(25)]\n",
    "#df = pd.DataFrame(pca.transform(train_df), columns=columns, index=df.index)\n",
    "#train_data, test_data = train_test_split(df, test_size = 0.3, random_state = seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_df.copy()\n",
    "test_data = test_df.copy()\n",
    "#train and test labels already created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#standardise data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(train_data)\n",
    "train_data = scaler.transform(train_data)\n",
    "test_data = scaler.transform(test_data)\n",
    "test_df = test_data.copy()\n",
    "train_df = train_data.copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithms for Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below relates to the ten different evaluation methods. The data has been standardised and the dimension reduced in the way as above, the data processed through each different method and evaluated before the results are printed and assessed. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Establish and validate model\n",
    "start_time = datetime.datetime.now()\n",
    "\n",
    "knn_baseline = KNeighborsClassifier(n_neighbors = 21)\n",
    "knn_baseline = knn_baseline.fit(train_data, train_labels)\n",
    "\n",
    "#knn_y_pred = knn_baseline.predict(test_data)\n",
    "knn_y_pred = cross_val_predict(knn_baseline, test_data, test_labels, cv = 5)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "exec_time = (end_time - start_time).total_seconds()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(knn_y_pred, test_labels)\n",
    "print(\"Accuracy is: {}\".format(accuracy))\n",
    "print(\"\\n\")\n",
    "\n",
    "print(classification_report(test_labels, knn_y_pred))\n",
    "\n",
    "auc = roc_auc_score(test_labels, knn_y_pred)\n",
    "print(\"ROC AUC is: {}\".format(auc))\n",
    "print(\"Execution time = {t:.3f} seconds\".format(t = exec_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Establish and validate model\n",
    "start_time = datetime.datetime.now()\n",
    "\n",
    "nb_baseline = GaussianNB()\n",
    "nb_baseline = nb_baseline.fit(train_data, train_labels)\n",
    "\n",
    "#nb_y_pred = nb_baseline.predict(test_data)\n",
    "nb_y_pred = cross_val_predict(nb_baseline, test_data, test_labels, cv = 5)\n",
    "\n",
    "end_time = datetime.datetime.now()\n",
    "exec_time = (end_time - start_time).total_seconds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(nb_y_pred, test_labels)\n",
    "print(\"Accuracy is: {}\".format(accuracy))\n",
    "print(\"\\n\")\n",
    "\n",
    "print(classification_report(test_labels, nb_y_pred))\n",
    "auc = roc_auc_score(test_labels, nb_y_pred)\n",
    "print(\"ROC AUC is: {}\".format(auc))\n",
    "\n",
    "print(\"Execution time = {t:.3f} seconds\".format(t = exec_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Establish and validate model\n",
    "start_time = datetime.datetime.now()\n",
    "\n",
    "svm_baseline = SVC(C=1500)\n",
    "svm_baseline = svm_baseline.fit(train_data, train_labels)\n",
    "\n",
    "#svm_y_pred = svm_baseline.predict(test_data)\n",
    "svm_y_pred = cross_val_predict(svm_baseline, test_data, test_labels, cv = 5)\n",
    "\n",
    "end_time = datetime.datetime.now()\n",
    "exec_time = (end_time - start_time).total_seconds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(svm_y_pred, test_labels)\n",
    "print(\"Accuracy is: {}\".format(accuracy))\n",
    "print(\"\\n\")\n",
    "print(classification_report(test_labels, svm_y_pred))\n",
    "\n",
    "auc = roc_auc_score(test_labels, svm_y_pred)\n",
    "print(\"ROC AUC is: {}\".format(auc))\n",
    "\n",
    "print(\"Execution time = {t:.3f} seconds\".format(t = exec_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = datetime.datetime.now()\n",
    "\n",
    "dt_baseline = DecisionTreeClassifier()\n",
    "dt_baseline = dt_baseline.fit(train_data, train_labels)\n",
    "\n",
    "#dt_y_pred = dt_baseline.predict(test_data)\n",
    "dt_y_pred = cross_val_predict(dt_baseline, test_data, test_labels, cv = 5)\n",
    "\n",
    "end_time = datetime.datetime.now()\n",
    "exec_time = (end_time - start_time).total_seconds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(dt_y_pred, test_labels)\n",
    "print(\"Accuracy is: {}\".format(accuracy))\n",
    "print(\"\\n\")\n",
    "\n",
    "print(classification_report(test_labels, dt_y_pred))\n",
    "\n",
    "auc = roc_auc_score(test_labels, dt_y_pred)\n",
    "print(\"ROC AUC is: {}\".format(auc))\n",
    "\n",
    "\n",
    "print(\"Execution time = {t:.3f} seconds\".format(t = exec_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Establish and validate model\n",
    "\n",
    "start_time = datetime.datetime.now()\n",
    "\n",
    "#rf_baseline = RandomForestClassifier(n_estimators = 100, min_impurity_decrease = 1.0)\n",
    "rf_baseline = RandomForestClassifier(n_estimators = 100)\n",
    "rf_baseline = rf_baseline.fit(train_data, train_labels)\n",
    "\n",
    "#rf_y_pred = rf_baseline.predict(test_data)\n",
    "rf_y_pred = cross_val_predict(rf_baseline, test_data, test_labels, cv = 5)\n",
    "\n",
    "end_time = datetime.datetime.now()\n",
    "exec_time = (end_time - start_time).total_seconds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(rf_y_pred, test_labels)\n",
    "print(\"Accuracy is: {}\".format(accuracy))\n",
    "print(\"\\n\")\n",
    "\n",
    "print(classification_report(test_labels, rf_y_pred))\n",
    "\n",
    "auc = roc_auc_score(test_labels, rf_y_pred)\n",
    "print(\"ROC AUC is: {}\".format(auc))\n",
    "\n",
    "print(\"Execution time = {t:.3f} seconds\".format(t = exec_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Establish and validate model\n",
    "start_time = datetime.datetime.now()\n",
    "ab_baseline = AdaBoostClassifier()\n",
    "ab_baseline = ab_baseline.fit(train_data, train_labels)\n",
    "\n",
    "#ab_y_pred = ab_baseline.predict(test_data)\n",
    "ab_y_pred = cross_val_predict(ab_baseline, test_data, test_labels, cv = 5)\n",
    "\n",
    "end_time = datetime.datetime.now()\n",
    "exec_time = (end_time - start_time).total_seconds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(ab_y_pred, test_labels)\n",
    "print(\"Accuracy is: {}\".format(accuracy))\n",
    "print(\"\\n\")\n",
    "\n",
    "print(classification_report(test_labels, ab_y_pred))\n",
    "\n",
    "auc = roc_auc_score(test_labels, ab_y_pred)\n",
    "print(\"ROC AUC is: {}\".format(auc))\n",
    "\n",
    "print(\"Execution time = {t:.3f} seconds\".format(t = exec_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Establish and validate model\n",
    "start_time = datetime.datetime.now()\n",
    "gb_baseline = GradientBoostingClassifier(learning_rate = 0.2, max_depth = 5, min_impurity_decrease = 0.1)\n",
    "gb_baseline = gb_baseline.fit(train_data, train_labels)\n",
    "\n",
    "#gb_y_pred = gb_baseline.predict(test_data)\n",
    "gb_y_pred = cross_val_predict(gb_baseline, test_data, test_labels, cv = 5)\n",
    "\n",
    "end_time = datetime.datetime.now()\n",
    "exec_time = (end_time - start_time).total_seconds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(gb_y_pred, test_labels)\n",
    "print(\"Accuracy is: {}\".format(accuracy))\n",
    "print(\"\\n\")\n",
    "\n",
    "print(classification_report(test_labels, gb_y_pred))\n",
    "\n",
    "auc = roc_auc_score(test_labels, gb_y_pred)\n",
    "print(\"ROC AUC is: {}\".format(auc))\n",
    "\n",
    "print(\"Execution time = {t:.3f} seconds\".format(t = exec_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Disciminant Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Establish and validate model\n",
    "start_time = datetime.datetime.now()\n",
    "\n",
    "lda_baseline = LinearDiscriminantAnalysis()\n",
    "lda_baseline = lda_baseline.fit(train_data, train_labels)\n",
    "\n",
    "#lda_y_pred = lda_baseline.predict(test_data)\n",
    "lda_y_pred = cross_val_predict(lda_baseline, test_data, test_labels, cv = 5)\n",
    "\n",
    "end_time = datetime.datetime.now()\n",
    "exec_time = (end_time - start_time).total_seconds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(lda_y_pred, test_labels)\n",
    "print(\"Accuracy is: {}\".format(accuracy))\n",
    "print(\"\\n\")\n",
    "\n",
    "print(classification_report(test_labels, lda_y_pred))\n",
    "\n",
    "auc = roc_auc_score(test_labels, lda_y_pred)\n",
    "print(\"ROC AUC is: {}\".format(auc))\n",
    "\n",
    "print(\"Execution time = {t:.3f} seconds\".format(t = exec_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Establish and validate model\n",
    "\n",
    "start_time = datetime.datetime.now()\n",
    "\n",
    "mlp_baseline = MLPClassifier(max_iter = 1000, solver = 'lbfgs', momentum = 0.2)\n",
    "mlp_baseline = mlp_baseline.fit(train_data, train_labels)\n",
    "\n",
    "#mlp_y_pred = mlp_baseline.predict(test_data)\n",
    "mlp_y_pred = cross_val_predict(mlp_baseline, test_data, test_labels, cv =  5)\n",
    "\n",
    "end_time = datetime.datetime.now()\n",
    "exec_time = (end_time - start_time).total_seconds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(mlp_y_pred, test_labels)\n",
    "print(\"Accuracy is: {}\".format(accuracy))\n",
    "print(\"\\n\")\n",
    "\n",
    "print(classification_report(test_labels, mlp_y_pred))\n",
    "\n",
    "auc = roc_auc_score(test_labels, mlp_y_pred)\n",
    "print(\"ROC AUC is: {}\".format(auc))\n",
    "\n",
    "print(\"Execution time = {t:.3f} seconds\".format(t = exec_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model#Establish and validate model\n",
    "start_time = datetime.datetime.now()\n",
    "lreg_baseline = LogisticRegression(C= 1500, class_weight = 'balanced')\n",
    "lreg_baseline = lreg_baseline.fit(train_data, train_labels)\n",
    "lreg_y_pred = lreg_baseline.predict(test_data)\n",
    "#lreg_y_pred = cross_val_predict(lreg_baseline, test_data, test_labels, cv = 5)\n",
    "\n",
    "end_time = datetime.datetime.now()\n",
    "exec_time = (end_time - start_time).total_seconds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(lreg_y_pred, test_labels)\n",
    "print(\"Accuracy is: {}\".format(accuracy))\n",
    "print(\"\\n\")\n",
    "\n",
    "print(classification_report(test_labels, lreg_y_pred))\n",
    "\n",
    "auc = roc_auc_score(test_labels, lreg_y_pred)\n",
    "print(\"ROC AUC is: {}\".format(auc))\n",
    "\n",
    "print(\"Execution time = {t:.3f} seconds\".format(t = exec_time))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
