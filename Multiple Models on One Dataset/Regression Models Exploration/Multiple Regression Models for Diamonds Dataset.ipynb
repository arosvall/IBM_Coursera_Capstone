{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.svm import LinearSVR\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import cross_val_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 309\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "df = pd.read_csv(\"diamonds.csv\")\n",
    "df.isnull().values.any()\n",
    "df = df.drop(df.columns[0], axis = 1)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this we can see that there is a high correlation between x, y, z and carat, while depth and table have a low correlation with the other variables as well as with the price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in df.columns:\n",
    "    print(column, \":\", df[column].unique(), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the processing changing the string values of cut, color and clarity into rankings. The way in which diamonds are valued allowed rankings rather than boolean values for the columns, with the highest rank being 1, the lowest being ~5, 6, 7 dependent on the number of unique values in the column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['cut']= np.where(df['cut'] == 'Fair', 5 , df['cut'])\n",
    "df['cut']= np.where(df['cut'] == 'Good', 4 , df['cut'])\n",
    "df['cut']= np.where(df['cut'] == 'Very Good', 3 , df['cut'])\n",
    "df['cut']= np.where(df['cut'] == 'Premium', 2 , df['cut'])\n",
    "df['cut']= np.where(df['cut'] == 'Ideal', 1 , df['cut'])\n",
    "\n",
    "df['color']= np.where(df['color'] == 'J', 7 , df['color'])\n",
    "df['color']= np.where(df['color'] == 'I', 6 , df['color'])\n",
    "df['color']= np.where(df['color'] == 'H', 5 , df['color'])\n",
    "df['color']= np.where(df['color'] == 'G', 4 , df['color'])\n",
    "df['color']= np.where(df['color'] == 'F', 3 , df['color'])\n",
    "df['color']= np.where(df['color'] == 'E', 2 , df['color'])\n",
    "df['color']= np.where(df['color'] == 'D', 1 , df['color'])\n",
    "\n",
    "df['clarity']= np.where(df['clarity'] == 'IF', 1 , df['clarity'])\n",
    "df['clarity']= np.where(df['clarity'] == 'VVS1', 2 , df['clarity'])\n",
    "df['clarity']= np.where(df['clarity'] == 'VVS2', 3 , df['clarity'])\n",
    "df['clarity']= np.where(df['clarity'] == 'VS1', 4 , df['clarity'])\n",
    "df['clarity']= np.where(df['clarity'] == 'VS2', 5 , df['clarity'])\n",
    "df['clarity']= np.where(df['clarity'] == 'SI1', 6 , df['clarity'])\n",
    "df['clarity']= np.where(df['clarity'] == 'SI2', 7 , df['clarity'])\n",
    "df['clarity']= np.where(df['clarity'] == 'I1', 8 , df['clarity'])\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in df.columns:\n",
    "    print(column, \":\", df[column].unique(), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.hist(bins = 10, figsize = (14, 10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The skew on the data, especially on the y and z values, could suggest a need for a log of those datas to get them into a more standard or gaussian distribution. However due to the low range of values in these columns (<60 for y and <30 for z) i decided to leave them in their current form before moving on to the splitting up of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Below the data is split into the training and test sets and processed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = train_test_split(df, test_size = 0.3, random_state = seed)\n",
    "train_data_full = train_data.copy()\n",
    "train_data = train_data.drop([\"price\"], axis = 1)\n",
    "train_labels = train_data_full[\"price\"]\n",
    "\n",
    "test_data_full = test_data.copy()\n",
    "test_data = test_data.drop([\"price\"], axis = 1)\n",
    "test_labels = test_data_full[\"price\"]\n",
    "\n",
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mean = train_data.mean()\n",
    "train_std = train_data.std()\n",
    "train_data = (train_data - train_mean) / train_std\n",
    "test_data = (test_data - train_mean) / train_std "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Below is all the regression techniques, code and outputs in order given in the assignment handout "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = datetime.datetime.now()\n",
    "\n",
    "lr_baseline = LinearRegression()\n",
    "lr_baseline.fit(train_data, train_labels)\n",
    "#lr_y_pred = lr_baseline.predict(test_data)\n",
    "lr_y_pred = cross_val_predict(lr_baseline, test_data, test_labels, cv = 10)\n",
    "\n",
    "\n",
    "end_time = datetime.datetime.now()\n",
    "exec_time = (end_time - start_time).total_seconds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Coefficients: \", lr_baseline.coef_)\n",
    "print(\"Intercept: \", lr_baseline.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = mean_squared_error(test_labels, lr_y_pred)\n",
    "print(\"MSE: {error}\".format(error=mse))\n",
    "\n",
    "print(\"RMSE: {error}\".format(error=np.sqrt(mse)))\n",
    "\n",
    "r2_error = r2_score(test_labels, lr_y_pred)\n",
    "print(\"R2: {error}\".format(error=r2_error))\n",
    "\n",
    "mae = mean_absolute_error(test_labels, lr_y_pred)\n",
    "print(\"MAE: {error}\".format(error=mae))\n",
    "\n",
    "print(\"Execution time = {t:.3f} seconds\".format(t = exec_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below code is all in relation to the KNN Regression task, reloading the data frame data into variable names, splitting the data and processing it before passing it through into the KNN evaluator before printing the four performance metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the KNN \n",
    "start_time = datetime.datetime.now()\n",
    "\n",
    "knn_baseline = KNeighborsRegressor(n_neighbors = 21, weights='distance')\n",
    "knn_baseline.fit(train_data, train_labels)\n",
    "#knn_y_pred = knn_baseline.predict(test_data)\n",
    "knn_y_pred = cross_val_predict(knn_baseline, test_data, test_labels, cv = 5)\n",
    "\n",
    "end_time = datetime.datetime.now()\n",
    "exec_time = (end_time - start_time).total_seconds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = mean_squared_error(test_labels, knn_y_pred)\n",
    "print(\"MSE: {error}\".format(error=mse))\n",
    "\n",
    "print(\"RMSE: {error}\".format(error=np.sqrt(mse)))\n",
    "\n",
    "knn_r2_error = r2_score(test_labels, knn_y_pred)\n",
    "print(\"R2: {error}\".format(error=knn_r2_error))\n",
    "\n",
    "mae = mean_absolute_error(test_labels, knn_y_pred)\n",
    "print(\"MAE: {error}\".format(error=mae))\n",
    "\n",
    "print(\"Execution time = {t:.3f} seconds\".format(t = exec_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge's Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code relates to the Ridges regression problem. It standardises the data before running the machine learning algorithm and prints out the performance metrics at the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = datetime.datetime.now()\n",
    "\n",
    "ridge_baseline = Ridge()\n",
    "ridge_baseline.fit(train_data, train_labels)\n",
    "#ridge_y_pred = ridge_baseline.predict(test_data)\n",
    "ridge_y_pred = cross_val_predict(ridge_baseline, test_data, test_labels, cv = 5)\n",
    "\n",
    "end_time = datetime.datetime.now()\n",
    "exec_time = (end_time - start_time).total_seconds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = mean_squared_error(test_labels, ridge_y_pred)\n",
    "print(\"MSE: {error}\".format(error=mse))\n",
    "\n",
    "print(\"RMSE: {error}\".format(error=np.sqrt(mse)))\n",
    "\n",
    "r2_error = r2_score(test_labels, ridge_y_pred)\n",
    "print(\"R2: {error}\".format(error=r2_error))\n",
    "\n",
    "mae = mean_absolute_error(test_labels, ridge_y_pred)\n",
    "print(\"MAE: {error}\".format(error=mae))\n",
    "\n",
    "print(\"Execution time = {t:.3f} seconds\".format(t = exec_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = datetime.datetime.now()\n",
    "\n",
    "dt_baseline = DecisionTreeRegressor(min_impurity_decrease = 1.0)\n",
    "dt_baseline.fit(train_data, train_labels)\n",
    "#dt_y_pred = dt_baseline.predict(test_data)\n",
    "dt_y_pred = cross_val_predict(dt_baseline, test_data, test_labels, cv = 5)\n",
    "\n",
    "end_time = datetime.datetime.now()\n",
    "exec_time = (end_time - start_time).total_seconds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mse = mean_squared_error(test_labels, dt_y_pred)\n",
    "print(\"MSE: {error}\".format(error=mse))\n",
    "\n",
    "print(\"RMSE: {error}\".format(error=np.sqrt(mse)))\n",
    "\n",
    "r2_error = r2_score(test_labels, dt_y_pred)\n",
    "print(\"R2: {error}\".format(error=r2_error))\n",
    "\n",
    "mae = mean_absolute_error(test_labels, dt_y_pred)\n",
    "print(\"MAE: {error}\".format(error=mae))\n",
    "\n",
    "print(\"Execution time = {t:.3f} seconds\".format(t = exec_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = datetime.datetime.now()\n",
    "\n",
    "rf_baseline = RandomForestRegressor(n_estimators = 20,\n",
    "min_impurity_decrease = 1.0)\n",
    "rf_baseline.fit(train_data, train_labels)\n",
    "#rf_y_pred = rf_baseline.predict(test_data)\n",
    "rf_y_pred = cross_val_predict(rf_baseline, test_data, test_labels, cv = 5)\n",
    "\n",
    "\n",
    "end_time = datetime.datetime.now()\n",
    "exec_time = (end_time - start_time).total_seconds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = mean_squared_error(test_labels, rf_y_pred)\n",
    "print(\"MSE: {error}\".format(error=mse))\n",
    "\n",
    "print(\"RMSE: {error}\".format(error=np.sqrt(mse)))\n",
    "\n",
    "r2_error = r2_score(test_labels, rf_y_pred)\n",
    "print(\"R2: {error}\".format(error=r2_error))\n",
    "\n",
    "mae = mean_absolute_error(test_labels, dt_y_pred)\n",
    "print(\"MAE: {error}\".format(error=mae))\n",
    "\n",
    "print(\"Execution time = {t:.3f} seconds\".format(t = exec_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = datetime.datetime.now()\n",
    "\n",
    "gb_baseline = GradientBoostingRegressor(learning_rate = 0.2, max_depth = 5, min_impurity_decrease = 0.1)\n",
    "gb_baseline = gb_baseline.fit(train_data, train_labels)\n",
    "#gb_y_pred = gb_baseline.predict(test_data)\n",
    "gb_y_pred = cross_val_predict(gb_baseline, test_data, test_labels, cv = 5)\n",
    "\n",
    "\n",
    "end_time = datetime.datetime.now()\n",
    "exec_time = (end_time - start_time).total_seconds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = mean_squared_error(test_labels, gb_y_pred)\n",
    "print(\"MSE: {error}\".format(error=mse))\n",
    "\n",
    "print(\"RMSE: {error}\".format(error=np.sqrt(mse)))\n",
    "\n",
    "r2_error = r2_score(test_labels, gb_y_pred)\n",
    "print(\"R2: {error}\".format(error=r2_error))\n",
    "\n",
    "mae = mean_absolute_error(test_labels, gb_y_pred)\n",
    "print(\"MAE: {error}\".format(error=mae))\n",
    "\n",
    "print(\"Execution time = {t:.3f} seconds\".format(t = exec_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGD Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = datetime.datetime.now()\n",
    "\n",
    "#Note, these are the base parameters that are being set to the standard values in order to avoid messages being printed about not having set these parameters and them defaulting to these settings\n",
    "sgd_baseline = SGDRegressor(tol = None, max_iter = 5)\n",
    "sgd_baseline = sgd_baseline.fit(train_data, train_labels)\n",
    "#sgd_y_pred = sgd_baseline.predict(test_data)\n",
    "sgd_y_pred = cross_val_predict(sgd_baseline, test_data, test_labels, cv = 5)\n",
    "\n",
    "\n",
    "end_time = datetime.datetime.now()\n",
    "exec_time = (end_time - start_time).total_seconds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = mean_squared_error(test_labels, sgd_y_pred)\n",
    "print(\"MSE: {error}\".format(error=mse))\n",
    "\n",
    "print(\"RMSE: {error}\".format(error=np.sqrt(mse)))\n",
    "\n",
    "r2_error = r2_score(test_labels, sgd_y_pred)\n",
    "print(\"R2: {error}\".format(error=r2_error))\n",
    "\n",
    "mae = mean_absolute_error(test_labels, sgd_y_pred)\n",
    "print(\"MAE: {error}\".format(error=mae))\n",
    "\n",
    "print(\"Execution time = {t:.3f} seconds\".format(t = exec_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = datetime.datetime.now()\n",
    "\n",
    "svr_baseline = SVR(C = 1500)\n",
    "svr_baseline = svr_baseline.fit(train_data, train_labels)\n",
    "#svr_y_pred = svr_baseline.predict(test_data)\n",
    "svr_y_pred = cross_val_predict(svr_baseline, test_data, test_labels, cv = 5)\n",
    "\n",
    "end_time = datetime.datetime.now()\n",
    "exec_time = (end_time - start_time).total_seconds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = mean_squared_error(test_labels, svr_y_pred)\n",
    "print(\"MSE: {error}\".format(error=mse))\n",
    "\n",
    "print(\"RMSE: {error}\".format(error=np.sqrt(mse)))\n",
    "\n",
    "r2_error = r2_score(test_labels, svr_y_pred)\n",
    "print(\"R2: {error}\".format(error=r2_error))\n",
    "\n",
    "mae = mean_absolute_error(test_labels, svr_y_pred)\n",
    "print(\"MAE: {error}\".format(error=mae))\n",
    "\n",
    "print(\"Execution time = {t:.3f} seconds\".format(t = exec_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = datetime.datetime.now()\n",
    "\n",
    "l_svr_baseline = LinearSVR(C = 1500)\n",
    "l_svr_baseline = l_svr_baseline.fit(train_data, train_labels)\n",
    "#l_svr_y_pred = l_svr_baseline.predict(test_data)\n",
    "l_svr_y_pred = cross_val_predict(l_svr_baseline, test_data, test_labels, cv = 5)\n",
    "\n",
    "end_time = datetime.datetime.now()\n",
    "exec_time = (end_time - start_time).total_seconds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = mean_squared_error(test_labels, l_svr_y_pred)\n",
    "print(\"MSE: {error}\".format(error=mse))\n",
    "\n",
    "print(\"RMSE: {error}\".format(error=np.sqrt(mse)))\n",
    "\n",
    "r2_error = r2_score(test_labels, l_svr_y_pred)\n",
    "print(\"R2: {error}\".format(error=r2_error))\n",
    "\n",
    "mae = mean_absolute_error(test_labels, l_svr_y_pred)\n",
    "print(\"MAE: {error}\".format(error=mae))\n",
    "\n",
    "print(\"Execution time = {t:.3f} seconds\".format(t = exec_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multilayer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = datetime.datetime.now()\n",
    "\n",
    "mlp_baseline = MLPRegressor(max_iter = 1000, solver = 'lbfgs', momentum = 0.2)\n",
    "mlp_baseline = mlp_baseline.fit(train_data, train_labels)\n",
    "#mlp_y_pred = mlp_baseline.predict(test_data)\n",
    "mlp_y_pred = cross_val_predict(mlp_baseline, test_data, test_labels, cv = 5)\n",
    "\n",
    "end_time = datetime.datetime.now()\n",
    "exec_time = (end_time - start_time).total_seconds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = mean_squared_error(test_labels, mlp_y_pred)\n",
    "print(\"MSE: {error}\".format(error=mse))\n",
    "\n",
    "print(\"RMSE: {error}\".format(error=np.sqrt(mse)))\n",
    "\n",
    "r2_error = r2_score(test_labels, mlp_y_pred)\n",
    "print(\"R2: {error}\".format(error=r2_error))\n",
    "\n",
    "mae = mean_absolute_error(test_labels, mlp_y_pred)\n",
    "print(\"MAE: {error}\".format(error=mae))\n",
    "\n",
    "print(\"Execution time = {t:.3f} seconds\".format(t = exec_time))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
